---
title: "Ridgereg(), caret and dplyr"
author: "Zaida Liendeborg, Emma Wallentinsson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette tackles about the **ridgereg()** function and a short description of methods implemented in the function. The vignette shows example of predictive modelling for **BostonHousing** from **mlbench** package using both **caret** and the **ridgereg()** function.   

## Ridgereg() function

The **ridgereg()** function fits a ridgre regression using the Ordinary Least Squares method. The function takes arguments **formula, data and lambda.** Formula should be the regression equation, it means to say both the response and the predictors. There should also be a dataset where the variables are taken from and a specified lambda value to be used to find the true parameters.

The function returns an list of a class **ridgereg** containing the regression equation used, the estimates of the beta coefficients and the fitted values. 

```{r, echo=FALSE}
library(multireg)
data(iris)
ridgeobject<- ridgereg(formula= Sepal.Length ~ Petal.Length + Petal.Width,
                       data=iris, lambda=0)
ridgeobject
```

There are three different methods implemented on the function and they will be discussed in the following sections:  

## The coef.ridgereg method

This **coef()** returns a name vector with the estimates of the beta coefficients from the fitted ridge regression. It only takes an object of a class ridgereg. 

```{r, echo=FALSE}
coef(ridgeobject)

```

## The print.ridgereg method

By using **print()**, the response variable and the explanatory variables used by the ridge regression model prints out. It also gives the estimates of the beta coefficients. See example below: 

```{r, echo=FALSE}
print(ridgeobject)

```

## The predict.ridgereg method

The **predict()** method simply gives the predicted values of the fitted ridge regression model. It takes two arguments such as **object and newdata**. Object should be a ridgereg object and newdata is an optional data frame in which the prediction of new values would be based on. If there is no given new dataset, the predict function will simply return a numeric vector with the fitted values. 

To show how predict works, let's take iris data as an example. First let's divide the data into two sets: **training and test** sets.

```{r}
data(iris)
train<- iris[1:75,]
test<- iris[76:150,]

pred_ridgeobject<- ridgereg(formula= Sepal.Length ~ Petal.Length + Petal.Width,
                       data=train, lambda=0)

new_pred<- predict(pred_ridgeobject, newdata=test)

head(new_pred)

```


## Predictive model for the BostonHousing data set

##1
A predictive model is created for the *BostonHousing* data set located in the **mlbench** package. First, the data is splitted in a training and test set using the **caret** package. The traning part is set to 75 % of the data. 
```{r}
library(mlbench)
library(leaps)
data("BostonHousing") #load data
library(caret)
inTrain <- createDataPartition(y = BostonHousing$medv,  p = .75, list = FALSE) #data partitioning

training <- BostonHousing[ inTrain,] #training set 
testing <- BostonHousing[-inTrain,] # test set 

```

##2

A linear regression model and a linear regression model with forward selection of covariates is fitted to the training set. 


```{r}

lmFit<-train(medv~., data = training, method="lm")# linear regression
lmFitForw<-train(medv~., data=training, method="leapForward")# forward selection
```

##3

The evaluating of the performance of the models are being done on the linear regression model: 

```{r, echo=FALSE}
#linear regression
predicted<- predict(lmFit)
modvalues<- data.frame(obs=training$medv, pred=predicted) #df with observed and predicted
defaultSummary(modvalues)

```

And on the fast forward regression model: 
```{r, echo=FALSE}
#fast forward regression 
predictedFF<- predict(lmFitForw)
modvaluesFF<- data.frame(obs=training$medv, pred=predictedFF) #df with observed and predicted
defaultSummary(modvaluesFF)

```

Which means that the linear regression model is better, due to the higher R square and lower RMSE.  

##4

Now we fit a ridge regression model with our **ridgereg** function, different values of lambda. 
```{r, eval=FALSE}
lpRidgeReg <- list(type = "Regression",
                   library = "multireg",
                   loop = NULL) 

prm <- data.frame(parameter = "lambda",
                  class = "numeric",
                  label = "lambda")


lpRidgeReg$parameters<-prm

grid <- function (x, y, len = NULL, search = "grid")
{
  if (search == "grid") {
    out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
  }
  else {
    out <- data.frame(lambda = 10^runif(len, min = -5, 1))
  }
  out
}

lpRidgeReg$grid <- grid

fitreg<-function(x,y,lambda,param,lev,last,classProbs,...){
  
  if(is.data.frame(x)){
    dat<- x
  }else{
    dat<- as.data.frame(x)
  } 
  dat$medv<- y

  frmla <- as.formula(paste(colnames(dat)[ncol(dat)], 
                            paste(colnames(dat)[1:(ncol(dat)-1)],
                                  sep = "", collapse = " + "), sep = " ~ "))
  
  model <- multireg::ridgereg(formula= frmla, data=dat, lambda= param)
  return(model)
}

lpRidgeReg$fit<-fitreg

pred <- function(modelFit, newdata, preProc = NULL, submodels = NULL){
  predict(modelFit, newdata)
}

lpRidgeReg$predict <- pred


lpRidgeReg$prob <- list(NULL)


##

#trainridge <- train(y=training$medv,x = training,method =lpRidgeReg)
# fungerar inte när vi använder vår egen modell... 
```


```{r}
train(form= medv~., data=training,method ="ridge")

```


This tells ut that the optimal value of lambda 0.

##5


Now, we are using 10 fold cross validation to find the best parameter lambda.  

```{r}
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
trainridgecv <- train(form= medv~., data=training,method ="ridge",
                       trControl = fitControl)
trainridgecv
```

The ultimate value for lambda here is still 0. 

##6


We test the models on our test data below

```{r}
testlm<-train(form=medv~., data=testing, method="lm")
testlm #RMSE 5,82 Rsq 0.621

testridge<-train(form=medv~., data= testing, method="ridge", lambda=0)
testridge #RMSE 5.49 Rsq 0.678
testridgecv <- train(form= medv~., data=testing,method ="ridge",
                       trControl = fitControl, lambda=0)
testridgecv # RMSE 5,13 Rsq 0.74


```

The best model according to RMSE and Rsquared is the ridge method with lambda =0 and the 10 fold cross validation. 


